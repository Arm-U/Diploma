{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d042ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "from openai import AzureOpenAI, AsyncAzureOpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "GPT_3_5_TURBO = \"gpt-3.5-turbo\"\n",
    "GPT_4_TURBO_PREVIEW = \"gpt-4-turbo-preview\"\n",
    "GPT_4 = 'gpt-4'\n",
    "GPT_4o = 'gpt-4o'\n",
    "\n",
    "def get_openai_api_key():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "    return os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-07-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abd3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "EVAL_FOLDER = '../../data/russian-english/cards/eval_results/ru_eng_'\n",
    "def get_eval_results_from_file(file_name):\n",
    "    data = []\n",
    "    file_path = EVAL_FOLDER + file_name\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f84c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = '../../data/russian-english/cards/test_cards/ru_eng_'\n",
    "def get_cards_from_file(file_name):\n",
    "    data = []\n",
    "    file_path = OUTPUT_FOLDER + file_name\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b73a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [f'sm1_new_kap{i}.json' for i in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046a7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_1 = get_cards_from_file(file_names[0])\n",
    "eval_results_1 = get_eval_results_from_file(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a44b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_only_phrase_cards(basic_cards, basic_eval_results, system_prompt, first_lang='Russian', second_lang='English', model=GPT_4o):\n",
    "    cards = []\n",
    "    eval_results = []\n",
    "    for basic_card, basic_eval_result in tqdm(list(zip(basic_cards, basic_eval_results))):\n",
    "        try:\n",
    "            word, sentence, _, _, card_id = basic_card.values()\n",
    "            if sentence != '':\n",
    "                cards.append(basic_card)\n",
    "                eval_results.append(basic_eval_result)\n",
    "                continue\n",
    "\n",
    "            user_prompt = f'''You will get simple word or phrase on Russian. Your task is to translate this word or phrase into English.\n",
    "            You don't need to translate the phrase literally. You need to translate it in such a way that the meaning of the phrase is preserved and the translation sounds natural.\n",
    "\n",
    "            === \n",
    "            Given word or phrase: '{word}'\n",
    "            ===\n",
    "\n",
    "\n",
    "            Return only english translated word or phrase and nothing else.\n",
    "            '''\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "              ]\n",
    "            )\n",
    "            \n",
    "            tr_word = response.choices[0].message.content\n",
    "\n",
    "            card = {}\n",
    "            card[\"wordFirstLang\"] = word\n",
    "            card[\"sentenceFirstLang\"] = \"\"\n",
    "            card[\"wordSecondLang\"] = tr_word.strip()\n",
    "            card[\"sentenceSecondLang\"] = \"\"\n",
    "            card[\"id\"] = card_id\n",
    "            cards.append(card)\n",
    "            \n",
    "            user_prompt = f\"\"\"\n",
    "            You are given a word or phrase in {first_lang}, along with its translation in {second_lang}. Your task is to evaluate the correctness of the translation.\n",
    "\n",
    "            You have to check whether the translation from {second_lang} to {first_lang} is accurate.\n",
    "\n",
    "            If the translation isn't accurate then suggest fix with explanation why you considered it as a mistake.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            In the suggestedFix fields don't provide explanations or instructions, just provide the final corrected string.\n",
    "            If it's better to fix word, not sentence, return null for suggestedFixSentence.\n",
    "\n",
    "\n",
    "            Provide a detailed evaluation for each point and suggest fixes where necessary.\n",
    "\n",
    "            Here is the words or phrases:\n",
    "            \n",
    "            ======\n",
    "            Word in {first_lang}: {card['wordFirstLang']}\n",
    "            ======\n",
    "            \n",
    "            ======\n",
    "            Word in {second_lang}: {card['wordSecondLang']}\n",
    "            ======\n",
    "\n",
    "            Respond in JSON format with the following structure:\n",
    "\n",
    "            {{\n",
    "              \"translationAccuracy\": {{\n",
    "                \"isCorrect\": true/false,\n",
    "                \"explanation\": \"Detailed explanation if there is an issue or why it's correct\",\n",
    "                \"suggestedFix\": \"Suggested correction if there is an issue\"\n",
    "              }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "              model=\"gpt-4o\",\n",
    "              response_format={ \"type\": \"json_object\" },\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "              ]\n",
    "            )\n",
    "\n",
    "            res = json.loads(response.choices[0].message.content.strip())\n",
    "            res['id'] = card['id']\n",
    "            eval_results.append(res)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e.message)\n",
    "    return cards, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707373ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52e1db302a24ced825b0ee3f9f3e40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = '''You are the Russian to English translater for beginners.'''\n",
    "lol, kek = improve_only_phrase_cards(cards_1[:22], eval_results_1[:22], system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2e5b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'wordFirstLang': 'Спокойной ночи! Спи крепко!',\n",
       "  'sentenceFirstLang': '',\n",
       "  'wordSecondLang': 'Good night! Sleep tight!',\n",
       "  'sentenceSecondLang': '',\n",
       "  'id': 1021},\n",
       " {'translationAccuracy': {'isCorrect': True,\n",
       "   'explanation': \"The translation 'Good night! Sleep tight!' accurately captures the meaning of the Russian phrase 'Спокойной ночи! Спи крепко!' Both convey a similar sentiment often used before sleeping, where 'Спокойной ночи!' translates to 'Good night!' and 'Спи крепко!' translates to 'Sleep tight!'\",\n",
       "   'suggestedFix': None},\n",
       "  'id': 1021})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[21], kek[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f83878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = '../../data/russian-english/cards/test_cards/ru_eng_'\n",
    "def write_cards_to_file(file_name, cards):\n",
    "    file_path = OUTPUT_FOLDER + file_name\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cards, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439bbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "EVAL_FOLDER = '../../data/russian-english/cards/eval_results/ru_eng_'\n",
    "def write_eval_results_to_file(file_name, results):\n",
    "    file_path = EVAL_FOLDER + file_name\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6a35e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa828b2e05854fa1801821de1f1a31a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab72958b44e4b72a11819fd74dd2971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d5ff07b7ca4e9eb496a6b7f95545dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb920d1b923a426693b879bcf5b8509f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8ff542e8bc4b5bb9c9f4d0840252a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf2e51c84d540e5835a6bd9fdc7ce72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2ece3682604d7a81fe3fbe750e6452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m             \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\httpx\\_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://pectoazureopenai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-07-01-preview'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3677982a89ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msystem_prompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'''You are the Russian to English translater for beginners.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfinal_cards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_eval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimprove_only_phrase_cards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mru_cards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msystem_prompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mwrite_cards_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_cards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-8a1ef707c0db>\u001b[0m in \u001b[0;36mimprove_only_phrase_cards\u001b[1;34m(basic_cards, basic_eval_results, system_prompt, first_lang, second_lang, model)\u001b[0m\n\u001b[0;32m     21\u001b[0m             '''\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             response = client.chat.completions.create(\n\u001b[0m\u001b[0;32m     24\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m               messages=[\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_utils\\_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\resources\\chat\\completions.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    740\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m    741\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         return self._post(\n\u001b[0m\u001b[0;32m    743\u001b[0m             \u001b[1;34m\"/chat/completions\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m             body=maybe_transform(\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         )\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     def patch(\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         return self._request(\n\u001b[0m\u001b[0;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m                 return self._retry_request(\n\u001b[0m\u001b[0;32m   1044\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Anaconda3\\lib\\site-packages\\openai\\_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1088\u001b[0m         \u001b[1;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# different thread if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m         return self._request(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_names = [f'sm1_new_kap{i}.json' for i in range(1, 8)]\n",
    "\n",
    "for file_name in tqdm(file_names):\n",
    "    ru_cards = get_cards_from_file(file_name)\n",
    "    eval_results = get_eval_results_from_file(file_name)\n",
    "    \n",
    "    system_prompt = '''You are the Russian to English translater for beginners.'''\n",
    "    final_cards, final_eval_results = improve_only_phrase_cards(ru_cards, eval_results, system_prompt)\n",
    "    \n",
    "    write_cards_to_file(file_name, final_cards)\n",
    "    \n",
    "    write_eval_results_to_file(file_name, final_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "196885d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e1e96f2c2d415b94218b9890316e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4ebf8f3b614420b7718ad263ed24e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = [f'sm1_new_kap{i}.json' for i in range(8, 9)]\n",
    "\n",
    "for file_name in tqdm(file_names):\n",
    "    ru_cards = get_cards_from_file(file_name)\n",
    "    eval_results = get_eval_results_from_file(file_name)\n",
    "    \n",
    "    system_prompt = '''You are the Russian to English translater for beginners.'''\n",
    "    final_cards, final_eval_results = improve_only_phrase_cards(ru_cards, eval_results, system_prompt)\n",
    "    \n",
    "    write_cards_to_file(file_name, final_cards)\n",
    "    \n",
    "    write_eval_results_to_file(file_name, final_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_only_phrase_cards_v2(basic_cards, basic_eval_results, system_prompt, model=GPT_4o):\n",
    "    cards = []\n",
    "    eval_results = []\n",
    "    for basic_card, basic_eval_result in tqdm(list(zip(basic_cards, basic_eval_results))):\n",
    "        try:\n",
    "            word, sentence, _, _, card_id = basic_card.values()\n",
    "            if sentence != '':\n",
    "                cards.append(basic_card)\n",
    "                eval_results.append(basic_eval_result)\n",
    "                continue\n",
    "                \n",
    "            system_prompt = '''You are a multilingual assistant who is proficient in Russian, Finnish and English.'''\n",
    "\n",
    "            user_prompt = f'''You will get simple word or phrase on Russian. Your task is to translate this word or phrase into English.\n",
    "            You don't need to translate the phrase literally. You need to translate it in such a way that the meaning of the phrase is preserved and the translation sounds natural.\n",
    "\n",
    "            === \n",
    "            Given word or phrase: '{word}'\n",
    "            ===\n",
    "\n",
    "\n",
    "            Return only english translated word or phrase and nothing else.\n",
    "            '''\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "              model=model,\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "              ]\n",
    "            )\n",
    "            \n",
    "            tr_word = response.choices[0].message.content\n",
    "\n",
    "            card = {}\n",
    "            card[\"wordFirstLang\"] = word\n",
    "            card[\"sentenceFirstLang\"] = \"\"\n",
    "            card[\"wordSecondLang\"] = tr_word.strip()\n",
    "            card[\"sentenceSecondLang\"] = \"\"\n",
    "            card[\"id\"] = card_id\n",
    "            cards.append(card)\n",
    "            \n",
    "            user_prompt = f\"\"\"\n",
    "            You are given a word or phrase in {first_lang}, along with its translation in {second_lang}. Your task is to evaluate the correctness of the translation.\n",
    "\n",
    "            You have to check whether the translation from {second_lang} to {first_lang} is accurate.\n",
    "\n",
    "            If the translation isn't accurate then suggest fix with explanation why you considered it as a mistake.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            In the suggestedFix fields don't provide explanations or instructions, just provide the final corrected string.\n",
    "            If it's better to fix word, not sentence, return null for suggestedFixSentence.\n",
    "\n",
    "\n",
    "            Provide a detailed evaluation for each point and suggest fixes where necessary.\n",
    "\n",
    "            Here is the words or phrases:\n",
    "            \n",
    "            ======\n",
    "            Word in {first_lang}: {card['wordFirstLang']}\n",
    "            ======\n",
    "            \n",
    "            ======\n",
    "            Word in {second_lang}: {card['wordSecondLang']}\n",
    "            ======\n",
    "\n",
    "            Respond in JSON format with the following structure:\n",
    "\n",
    "            {{\n",
    "              \"translationAccuracy\": {{\n",
    "                \"isCorrect\": true/false,\n",
    "                \"explanation\": \"Detailed explanation if there is an issue or why it's correct\",\n",
    "                \"suggestedFix\": \"Suggested correction if there is an issue\"\n",
    "              }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "              model=\"gpt-4o\",\n",
    "              response_format={ \"type\": \"json_object\" },\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "              ]\n",
    "            )\n",
    "\n",
    "            res = json.loads(response.choices[0].message.content.strip())\n",
    "            res['id'] = card['id']\n",
    "            eval_results.append(res)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e.message)\n",
    "    return cards, eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
